{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17302eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import PIL.Image as Image\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, Normalize,Compose, ToTensor,RandomCrop,RandomHorizontalFlip,ColorJitter,Compose\n",
    "import random\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import scipy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch.nn.functional as F\n",
    "# some parts of code is inspired by Zhang et.al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a450cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(2022)\n",
    "np.random.seed(2022)\n",
    "random.seed(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "044f375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization\n",
    "main_path='C:/Users/sanaz/Desktop/OSLOMET/Semester 2/Data mining/final project/cifar100/'\n",
    "splits_path = main_path+'splits/bertinetto/'\n",
    "data_path=main_path+'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a83fbdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_classes(split):\n",
    "    file_path=os.path.join(splits_path,split)\n",
    "    with open(file_path) as f:\n",
    "        return f.read().splitlines() \n",
    "\n",
    "train_classes=read_classes('train.txt')\n",
    "val_classes=read_classes('val.txt')\n",
    "test_classes=read_classes('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddc78516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "16\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(train_classes))\n",
    "print(len(val_classes))\n",
    "print(len(test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef3ee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_paths(classes):\n",
    "    paths=[]\n",
    "    labels=[]\n",
    "    for i,class_name in enumerate(classes):\n",
    "\n",
    "        for img_path in glob.glob(data_path +class_name+ '/*'):\n",
    "            paths.append(img_path)\n",
    "            labels.append(i)\n",
    "    return paths,labels\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f97f6",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1a3e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class base_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, main_path,training,size):\n",
    "        \n",
    "        all_train_classes=read_classes('train.txt')\n",
    "        train_classes=all_train_classes[:20].copy()\n",
    "        train_paths,train_labels=read_paths(train_classes)\n",
    "        train_tuples=list(zip(train_paths,train_labels))\n",
    "        random.shuffle(train_tuples)\n",
    "\n",
    "        train_paths,train_labels = zip(*train_tuples)\n",
    "        num=int(len(train_paths)*0.9)\n",
    "        \n",
    "        \n",
    "        if training:\n",
    "            self.paths=train_paths[:num]\n",
    "            self.labels=train_labels[:num]\n",
    "            self.transform=Compose([ Resize((size, size)),\n",
    "                                                RandomCrop(size, padding=8),\n",
    "                                                ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "                                                RandomHorizontalFlip(),\n",
    "                                                ToTensor(),\n",
    "                                                Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                                                ])\n",
    "        else:\n",
    "            self.paths=train_paths[num:]\n",
    "       \n",
    "            self.labels=train_labels[num:]\n",
    "            self.transform=Compose([ Resize((size, size)),\n",
    "                                                ToTensor(),\n",
    "                                                Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                                ])\n",
    "        self.length=len(self.paths)\n",
    "    def __getitem__(self, idx): \n",
    "        \n",
    "        img=Image.open(self.paths[idx]).convert('RGB')\n",
    "        img=self.transform(img)\n",
    "        return img,self.labels[idx]\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7732515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, size):\n",
    "        \n",
    "        all_test_classes=read_classes('test.txt')\n",
    "        self.test_classes=all_test_classes[:10].copy()\n",
    "        self.test_paths,self.test_labels=read_paths(test_classes)\n",
    "        self.size=size\n",
    "        self.length=len(self.test_paths)\n",
    "        self.transform = transforms.Compose([\n",
    "                                               Resize((size, size)),\n",
    "                                               ToTensor(),\n",
    "                                               Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "        \n",
    "    def __getitem__(self, idx): \n",
    "        if idx == -1:\n",
    "            return torch.zeros([3, self.size, self.size]), 0\n",
    "        img=Image.open(self.test_paths[idx]).convert('RGB')\n",
    "        img=self.transform(img)\n",
    "        return img,1\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7c08276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class testSampler():\n",
    "\n",
    "    def __init__(self, lbl, batch_no ):\n",
    "        self.batch_no = batch_no\n",
    "        self.class_no = 5\n",
    "        self.no_per_class = 35\n",
    "        self.number_distract = 15\n",
    "\n",
    "        lbl = np.array(lbl)\n",
    "        self.ind = []\n",
    "        for i in range(max(lbl) + 1):\n",
    "            ind = np.argwhere(lbl == i).reshape(-1)\n",
    "            ind = torch.from_numpy(ind)\n",
    "            self.ind.append(ind)\n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for b in range(self.batch_no):\n",
    "            \n",
    "            classes = torch.randperm(len(self.ind))\n",
    "            batch_ind = []\n",
    "            batch = []\n",
    "            \n",
    "            for c in classes[:self.class_no]:\n",
    "                lb = self.ind[c]\n",
    "                pos = torch.randperm(len(lb))[:self.no_per_class]\n",
    "                batch_cls = lb[pos]\n",
    "                ind_class = np.zeros(self.no_per_class)\n",
    "                ind_class[:batch_cls.shape[0]] = 1\n",
    "                if batch_cls.shape[0] != self.no_per_class:\n",
    "                    batch_cls = torch.cat([batch_cls, -1*torch.ones([self.no_per_class-batch_cls.shape[0]]).long()], 0)\n",
    "                batch.append(batch_cls)\n",
    "                batch_ind.append(ind_class)\n",
    "            batch = torch.stack(batch).t().reshape(-1)\n",
    "            yield batch\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.batch_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d1f89f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, in_chnls, out_chnls, stride=1):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_chnls, out_chnls, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.batchn1 = nn.BatchNorm2d(out_chnls)\n",
    "        self.conv2 = nn.Conv2d(out_chnls, out_chnls, kernel_size=3,stride=1, padding=1, bias=False)\n",
    "        self.batchn2 = nn.BatchNorm2d(out_chnls)\n",
    "\n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or in_chnls != out_chnls:\n",
    "            self.skip = nn.Sequential(nn.Conv2d(in_chnls, out_chnls,kernel_size=1, stride=stride, bias=False),\n",
    "                                      nn.BatchNorm2d(out_chnls))\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.batchn1(self.conv1(x)))\n",
    "        out = self.batchn2(self.conv2(out))\n",
    "        out += self.skip(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "class Resnet18(nn.Module):\n",
    "    def __init__(self,in_chnls,no_classes,resnet_block):\n",
    "        super(Resnet18,self).__init__()\n",
    "        self.in_chnls=in_chnls\n",
    "        self.out_chnls=64\n",
    "        self.conv1 = nn.Conv2d(in_chnls, self.out_chnls, kernel_size=3,stride=1, padding=1, bias=False)\n",
    "        self.batchn = nn.BatchNorm2d(64)\n",
    "        self.l1 = self.build_layer(resnet_block, 64, stride=1)\n",
    "        self.l2 = self.build_layer(resnet_block, 128, stride=2)\n",
    "        self.l3 = self.build_layer(resnet_block, 256, stride=2)\n",
    "        self.l4 = self.build_layer(resnet_block, 512, stride=2)\n",
    "        self.fc=nn.Linear(512,no_classes)\n",
    "        \n",
    "    def build_layer(self, block, chnls, stride):\n",
    "        layers = []\n",
    "        strides = [stride] + [1]\n",
    "        \n",
    "        for stride in strides:\n",
    "            layers.append(block(self.out_chnls, chnls, stride))\n",
    "            self.out_chnls = chnls\n",
    "        return nn.Sequential(*layers)          \n",
    "              \n",
    "    \n",
    "    def forward(self,x,test=False):\n",
    "        \n",
    "        \n",
    "        x=self.conv1(x)\n",
    "        x=self.batchn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        x = F.avg_pool2d(x, 4)\n",
    "        x=x.reshape(x.shape[0],-1)\n",
    "        if test: return x\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4181461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_backbone():\n",
    "    \n",
    "    #initializing hyperprameters\n",
    "    \n",
    "    learning_rate=0.01\n",
    "    momentum=0.9\n",
    "    weight_decay=1e-4\n",
    "    num_classes = 20\n",
    "    optim_step_size=30\n",
    "    optim_gamma=0.1\n",
    "    num_epochs=40  \n",
    "    \n",
    "    #read data\n",
    "    base_train_dataset=base_dataset(main_path,training=True,size=32)\n",
    "    base_train_loader = DataLoader(base_train_dataset,  batch_size=64,shuffle=True)\n",
    "    base_val_dataset=base_dataset(main_path,training=False,size=32)\n",
    "    base_val_loader = DataLoader(base_val_dataset,  batch_size=32,shuffle=False)\n",
    "        \n",
    "    model = Resnet18(3,num_classes,ResnetBlock)\n",
    "    \n",
    "    # Define the optimizer and loss function\n",
    "    \n",
    "    optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)   \n",
    "    scheduler = StepLR(optim, optim_step_size, gamma=optim_gamma)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    best= 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        scheduler.step()\n",
    "        tr_losses = []\n",
    "        tr_acc = []\n",
    "        \n",
    "        for imgs, lbls in tqdm(base_train_loader):\n",
    "            tr_pred = model(imgs)\n",
    "            loss = criterion(tr_pred, lbls)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            tr_losses.append(loss.item())\n",
    "            tr_acc.append(tr_pred.max(1)[1].eq(lbls).float().mean().item())\n",
    "        val_acc = []\n",
    "        model.eval()\n",
    "        for imgs, lbls in base_val_loader:\n",
    "            val_pred = model(imgs)\n",
    "            val_pred = torch.argmax(val_pred, 1).reshape(-1)\n",
    "            lbls = lbls.reshape(-1)\n",
    "            val_acc += (val_pred==lbls).tolist()\n",
    "        val_acc = np.mean(val_acc)\n",
    "        print(f'epoch:{epoch}, training loss:{np.mean(tr_losses)},training accuracy:{np.mean(tr_acc)}, validation-accuracy:{val_acc}')\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6bf9c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [07:14<00:00,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, training loss:2.62016238545525,training accuracy:0.19834812629152332, validation-accuracy:0.30083333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [08:27<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, training loss:2.2796216645889733,training accuracy:0.2874137080280033, validation-accuracy:0.3908333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [08:44<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2, training loss:2.1064731637401692,training accuracy:0.342239891459956, validation-accuracy:0.4033333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [11:08<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3, training loss:1.9436858405728312,training accuracy:0.39127218934911245, validation-accuracy:0.43333333333333335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [09:23<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4, training loss:1.8123803427938878,training accuracy:0.4402120316169671, validation-accuracy:0.4175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [08:52<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5, training loss:1.6712936889490433,training accuracy:0.4780880177514793, validation-accuracy:0.5008333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [09:24<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6, training loss:1.5193449791366531,training accuracy:0.53125, validation-accuracy:0.5983333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [08:26<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7, training loss:1.3805347829175418,training accuracy:0.5616987178311545, validation-accuracy:0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [08:31<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8, training loss:1.286043105746162,training accuracy:0.5954450196063025, validation-accuracy:0.6383333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [08:56<00:00,  3.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9, training loss:1.1891794356368703,training accuracy:0.6236748028789046, validation-accuracy:0.6133333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [08:36<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10, training loss:1.1225499803497947,training accuracy:0.6468503450500894, validation-accuracy:0.7033333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [08:36<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11, training loss:1.0373569695201852,training accuracy:0.6719982740441723, validation-accuracy:0.7008333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [07:37<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12, training loss:0.9872692569473086,training accuracy:0.6910133136094675, validation-accuracy:0.7383333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:55<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13, training loss:0.9063086181702699,training accuracy:0.713233481144764, validation-accuracy:0.7383333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:47<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14, training loss:0.8507826419977041,training accuracy:0.7277490136891427, validation-accuracy:0.6791666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:47<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15, training loss:0.8121188953078005,training accuracy:0.7408468934911243, validation-accuracy:0.8016666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:38<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16, training loss:0.7686835989444214,training accuracy:0.7608173076923077, validation-accuracy:0.8075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:32<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17, training loss:0.7306458997655902,training accuracy:0.7660564596836383, validation-accuracy:0.7691666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:34<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18, training loss:0.68245648385505,training accuracy:0.7801097141215082, validation-accuracy:0.7958333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:31<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19, training loss:0.6650139208023365,training accuracy:0.787444526627219, validation-accuracy:0.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:30<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20, training loss:0.6436292197577347,training accuracy:0.795888806588551, validation-accuracy:0.7975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:31<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21, training loss:0.5993769299349135,training accuracy:0.8102502466658869, validation-accuracy:0.7958333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:31<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22, training loss:0.5762139141735946,training accuracy:0.8137327414997936, validation-accuracy:0.8533333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:34<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23, training loss:0.5417249600386479,training accuracy:0.8257211538461539, validation-accuracy:0.8108333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:32<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24, training loss:0.533044936653425,training accuracy:0.8253205129380762, validation-accuracy:0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:32<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25, training loss:0.48407608332365926,training accuracy:0.8419625247723957, validation-accuracy:0.8683333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:25<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26, training loss:0.4827589916407004,training accuracy:0.8416543391329296, validation-accuracy:0.8491666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:38<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27, training loss:0.4492290725369425,training accuracy:0.8539201183431953, validation-accuracy:0.8891666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [07:06<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28, training loss:0.45702546737955874,training accuracy:0.8504684419321591, validation-accuracy:0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [07:04<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29, training loss:0.30876417277303675,training accuracy:0.9019662229972478, validation-accuracy:0.9475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [33:08<00:00, 11.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30, training loss:0.2624849971582198,training accuracy:0.9147250987368928, validation-accuracy:0.9475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [05:28<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31, training loss:0.2304660670856047,training accuracy:0.9256348620505023, validation-accuracy:0.9483333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:50<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32, training loss:0.22340782700911077,training accuracy:0.9286242603550295, validation-accuracy:0.9508333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:47<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33, training loss:0.22196836315492202,training accuracy:0.9259738657601486, validation-accuracy:0.9558333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:36<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34, training loss:0.2076198523919258,training accuracy:0.9341715976331361, validation-accuracy:0.9608333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:21<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35, training loss:0.19951117827871143,training accuracy:0.9364829881656804, validation-accuracy:0.9566666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:46<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36, training loss:0.1986388562787214,training accuracy:0.9350653354232833, validation-accuracy:0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [07:13<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37, training loss:0.1884184495584499,training accuracy:0.9401503945948809, validation-accuracy:0.9591666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:56<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38, training loss:0.1785727602887083,training accuracy:0.9440027119139948, validation-accuracy:0.9658333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 169/169 [06:20<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39, training loss:0.18238546183476081,training accuracy:0.9405510355029586, validation-accuracy:0.96\n"
     ]
    }
   ],
   "source": [
    "model=train_backbone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c98ec642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(model, data):\n",
    "    \n",
    "    if data.shape[0] > 64:\n",
    "        feat = []\n",
    "        count= 0\n",
    "        while count <= data.shape[0]-1:\n",
    "            feat.append(model(data[count:count+64], test=True).detach())\n",
    "            count += 64\n",
    "        feat = torch.cat(feat)\n",
    "    else:\n",
    "        feat = model(data, test=True).detach()\n",
    "    \n",
    "    return feat.numpy()\n",
    "\n",
    "def update_sup( sup_set, X_hat, y_hat, no_sup, pseudoy):\n",
    "        net = ElasticNet(l1_ratio=1.0,alpha=1.0, normalize=True, fit_intercept=True,selection='cyclic', warm_start=True)\n",
    "   \n",
    "        _, coefs, _ = net.path(X_hat, y_hat, l1_ratio=1.0)\n",
    "        coefs = np.sum(np.abs(coefs.transpose(2, 1, 0)[::-1, no_sup:, :]), axis=2)\n",
    "        sel = np.zeros(5)\n",
    "        for coef in coefs:\n",
    "            for i, c in enumerate(coef):\n",
    "                if c == 0.0 and (i+no_sup not in sup_set) and (sel[pseudoy[i]] < 1):    \n",
    "                    sup_set.append(i+no_sup)\n",
    "                    sel[pseudoy[i]] += 1\n",
    "            if np.sum(sel >= 1) == 5:\n",
    "                break\n",
    "        return sup_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "075b2150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_acc( sup_X, sup_y, query_X, unlabel_X, query_y):\n",
    "\n",
    "    no_sup =  len(sup_X)\n",
    "    no_unlabel = unlabel_X.shape[0]\n",
    "    \n",
    "    \n",
    "    sup_unlabel_feat = np.concatenate([sup_X, unlabel_X])\n",
    "    pca=PCA(n_components=5)  \n",
    "    X = pca.fit_transform(sup_unlabel_feat)\n",
    "    \n",
    "    H = np.dot(np.dot(X, np.linalg.inv(np.dot(X.T, X))), X.T)\n",
    "    X_hat = np.eye(H.shape[0]) - H\n",
    "\n",
    "    sup_set = np.arange(no_sup).tolist()\n",
    "    classifier = LogisticRegression(C=10,solver='lbfgs', multi_class='auto', max_iter=1000)\n",
    "    classifier.fit(sup_X, sup_y)\n",
    "    accs = []\n",
    "    for _ in range(no_sup + no_unlabel):\n",
    " \n",
    "        pseudoy = classifier.predict(unlabel_X)\n",
    "        y = np.concatenate([sup_y, pseudoy])\n",
    "        Y = np.zeros((y.shape[0], 5))\n",
    "        for i, n in enumerate(y):\n",
    "            Y[i, n] = 1.0\n",
    "        \n",
    "        y_hat = np.dot(X_hat, Y)\n",
    "        sup_set = update_sup(sup_set, X_hat, y_hat, no_sup, pseudoy)\n",
    "        y = np.argmax(Y, axis=1)\n",
    "        classifier.fit(sup_unlabel_feat[sup_set], y[sup_set])\n",
    "        if len(sup_set) == len(sup_unlabel_feat):\n",
    "            break\n",
    "    preds = classifier.predict(query_X)\n",
    "    accs.append(np.mean(preds == query_y))\n",
    "    return accs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41d4f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(model):\n",
    "    \n",
    "    model.eval()                            \n",
    "    num_batches=20\n",
    "    \n",
    "    #read test data                                 \n",
    "    testdataset=test_dataset(size=32)\n",
    "    sampler = testSampler(testdataset.test_labels, num_batches)\n",
    "    test_loader = DataLoader(testdataset, batch_sampler=sampler,shuffle=False,  pin_memory=True)\n",
    "    \n",
    "                            \n",
    "    \n",
    "                            \n",
    "    for data, indicator in tqdm(test_loader):\n",
    "        targets = torch.arange(5).repeat(35).long()[indicator[:175] != 0]\n",
    "                            \n",
    "        data = data[indicator != 0]\n",
    "         \n",
    "        train_targets = targets[:25].numpy()\n",
    "        test_targets = targets[25:100].numpy()\n",
    "                            \n",
    "        train_feat = get_features(model, data[:25])\n",
    "        sup_X=normalize( train_feat ) \n",
    "        sup_y=train_targets\n",
    "                            \n",
    "        test_feat = get_features(model, data[25:100])\n",
    "        \n",
    "        unlabel_feat = get_features(model, data[100:])\n",
    "        query_feat = normalize(test_feat)\n",
    "        unlabel_feat = normalize(unlabel_feat)                    \n",
    "        acc = get_acc(sup_X,sup_y,query_feat, unlabel_feat, test_targets)\n",
    "                         \n",
    "       \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "edc123d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [01:07<00:00,  3.36s/it]\n"
     ]
    }
   ],
   "source": [
    "accuracy=test_classifier(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9d288456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query mean accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "print('Query mean accuracy:',np.mean(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa792f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5349a1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c585f91d3623973be3accc48b0d5e967ce904a396a0f0c8bda7b100d8b60333f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
